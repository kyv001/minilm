{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from config import *\n",
    "from encoder import Encoder\n",
    "from modules_rwkv import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_lr(log_file):\n",
    "    with open(log_file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    losses = []\n",
    "    lrs = []\n",
    "    for line in lines:\n",
    "        step = int(line.split()[0][4:-1])\n",
    "        loss = float(line.split()[1][5:])\n",
    "        lr = float(line.split()[3][3:])\n",
    "        if len(losses) > step - 1:\n",
    "            losses[step - 1] = loss\n",
    "            lrs[step - 1] = lr\n",
    "        else:\n",
    "            losses.append(loss)\n",
    "            lrs.append(lr)\n",
    "    return losses, lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import BinaryDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(\n",
    "    BinaryDataset(\"part-2021278643.json.bin\", MAX_LENGTH),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "encoder = Encoder.from_path(\"encoder.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(encoder.vocab_size, MODEL_DIM, LORA_DIM, N_BLOCKS, N_HEADS).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = MAX_LENGTH\n",
    "for x, y, n_tokens in loader:\n",
    "    x = x.to(\"cuda\")[:, :length, ...]\n",
    "    y = y.to(\"cuda\")[:, :length, ...]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78255616 parameters.\n",
      "torch.Size([1, 1024, 768])\n",
      "torch.Size([1, 1024, 768])\n",
      "torch.Size([1, 1024, 768])\n",
      "torch.Size([1, 1024, 768])\n",
      "torch.Size([1, 1024, 768])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 5.70 GiB of which 2.38 MiB is free. Process 11344 has 57.60 MiB memory in use. Including non-PyTorch memory, this process has 5.57 GiB memory in use. Of the allocated memory 5.23 GiB is allocated by PyTorch, and 216.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(para\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mpara\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mllm\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_float32_matmul_precision(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m logits, state \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/data/编程/LLM/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/编程/LLM/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/编程/LLM/modules_rwkv.py:168\u001b[0m, in \u001b[0;36mLLM.forward\u001b[0;34m(self, x, states)\u001b[0m\n\u001b[1;32m    166\u001b[0m     states_i \u001b[38;5;241m=\u001b[39m states \u001b[38;5;66;03m# type: ignore # 此处的states不可能是None，而是list[torch.Tensor]\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)):\n\u001b[0;32m--> 168\u001b[0m     x, s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     states1\u001b[38;5;241m.\u001b[39mappend(s)\n\u001b[1;32m    170\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x)\n",
      "File \u001b[0;32m/data/编程/LLM/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/编程/LLM/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/编程/LLM/modules_rwkv.py:138\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, state: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 138\u001b[0m     x1, state1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_mixing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     x \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m+\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_mixing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, state1\n",
      "File \u001b[0;32m/data/编程/LLM/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/编程/LLM/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/编程/LLM/modules_rwkv.py:102\u001b[0m, in \u001b[0;36mTimeMixing.forward\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m    100\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_proj(x \u001b[38;5;241m+\u001b[39m xxx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_lora(lerpx))\n\u001b[1;32m    101\u001b[0m w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_lora(x \u001b[38;5;241m+\u001b[39m xxx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_lora(lerpx))))\n\u001b[0;32m--> 102\u001b[0m out, state1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, state1\n",
      "File \u001b[0;32m/data/编程/LLM/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/编程/LLM/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/编程/LLM/modules_rwkv.py:59\u001b[0m, in \u001b[0;36mMultiHeadWKV.forward\u001b[0;34m(self, r, w, k, v, g, state)\u001b[0m\n\u001b[1;32m     57\u001b[0m     kv \u001b[38;5;241m=\u001b[39m k[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i, :]\u001b[38;5;241m.\u001b[39mmT \u001b[38;5;241m@\u001b[39m v[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i, :]\n\u001b[1;32m     58\u001b[0m     wkv[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i, :, :] \u001b[38;5;241m=\u001b[39m state1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiag(u) \u001b[38;5;241m@\u001b[39m kv\n\u001b[0;32m---> 59\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\n\u001b[1;32m     60\u001b[0m o \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(g) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln((r\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m@\u001b[39m wkv)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# (B, H, T, C/H) -> (B, T, H, C/H) -> (B, T, C)\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 5.70 GiB of which 2.38 MiB is free. Process 11344 has 57.60 MiB memory in use. Including non-PyTorch memory, this process has 5.57 GiB memory in use. Of the allocated memory 5.23 GiB is allocated by PyTorch, and 216.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "print(f\"{sum(para.numel() for para in llm.parameters())} parameters.\")\n",
    "torch.set_float32_matmul_precision('high')\n",
    "logits, state = llm(x)\n",
    "loss = F.cross_entropy(logits.contiguous().view(-1, logits.size(-1)), y.contiguous().view(-1), reduction='mean', ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重庆通报3起违反中央八项规定精神问题：央广网重庆1月31日消息(记者吴新伟)重庆市纪委监察委官微\"风正巴渝\"今披露,该市近日通报3起违反中央八项规定精神问题。据通报,重庆长寿化工有限责任公司党委委员、董事、副总经理李文成违规发放津补贴。2013年4月至2017年11月,李文成违规以节日津补贴等名义向公司办公室职工发放11万余元,其中本人违规领取3.2万余元。2018年7月,李文成受到党内严重警告处分、扣减2017年全部绩效年薪处理；违纪款项已清退。垫江县原卫生和计划生育委员会党委书记、主任刘卫东违规收受礼金问题。2013年1月至2017年1月,刘卫东在任垫江县原卫生局党委书记、局长,以及垫江县原卫生和计划生育委员会党委书记、主任期间,先后收受管理服务对象礼金共计4万元；同时,刘卫东还存在其他严重违纪违法问题。2018年9月,刘卫东受到开除党籍、开除公职处分；其涉嫌犯罪问题已移送司法机关依法处理；违纪款项已收缴。时任梁平县市政园林管理局党委委员、副局长雷仁友等人公款旅游问题。2015年10月,雷仁友与时任县市政设施管理所所长王平波及该所规划科科长王斌,借参加培训之机,前往杭州、苏州等地景区旅游,并公款报销有关费用共计1万余元。2018年8月、9月,雷仁友受到党内严重警告处分,王平波、王斌均受到党内警告处分；因履行主体责任和监督责任不力,时任县市政园林管理局党委书记、局长李剑勤和时任县市政园林管理局纪委书记刘安俊被责令作出书面检查；违纪款项已收缴。<eos>真的勇!罗志祥生日全网只有他一个人送祝福,真是艺高人胆大：娱乐圈是一个很小的圈子,明星间总会有交集。罗志祥的\"多人运动\"把自己的名声以及事业彻底搞垮,还和周扬青分手,被粉丝和网友抵制,不得不说需要给罗志祥颁一个年度\"最惨\"艺人的奖。但是,这一切的事情的起因,和罗志祥的生活作风脱不了干系。这不,就算是知道罗志祥的遭到封杀,还是给罗志祥送上生日祝福的一个知名节目导演严敏,这个严敏是前四季《极限挑战》的总导演,看来严敏也是一个性情中人,就算知道罗志祥现在的状况,还是一如既往的选择支持罗志祥,不得不说,罗志祥还是人缘不错的,尤其是在事业上。其实,罗志祥要是没有伤害到周扬青的话,周扬青是不会主动曝出这些事的,对于周扬青而言,感情上的背叛是零容忍的,关键是罗志祥一直都在骗她,是个人都会忍不了。周扬青选择借助社交平台将这件事说出来,一方面的原因是这件事真的恶心到她了；另一方面的原因是她\n",
      "庆通报3起违反中央八项规定精神问题：央广网重庆1月31日消息(记者吴新伟)重庆市纪委监察委官微\"风正巴渝\"今披露,该市近日通报3起违反中央八项规定精神问题。据通报,重庆长寿化工有限责任公司党委委员、董事、副总经理李文成违规发放津补贴。2013年4月至2017年11月,李文成违规以节日津补贴等名义向公司办公室职工发放11万余元,其中本人违规领取3.2万余元。2018年7月,李文成受到党内严重警告处分、扣减2017年全部绩效年薪处理；违纪款项已清退。垫江县原卫生和计划生育委员会党委书记、主任刘卫东违规收受礼金问题。2013年1月至2017年1月,刘卫东在任垫江县原卫生局党委书记、局长,以及垫江县原卫生和计划生育委员会党委书记、主任期间,先后收受管理服务对象礼金共计4万元；同时,刘卫东还存在其他严重违纪违法问题。2018年9月,刘卫东受到开除党籍、开除公职处分；其涉嫌犯罪问题已移送司法机关依法处理；违纪款项已收缴。时任梁平县市政园林管理局党委委员、副局长雷仁友等人公款旅游问题。2015年10月,雷仁友与时任县市政设施管理所所长王平波及该所规划科科长王斌,借参加培训之机,前往杭州、苏州等地景区旅游,并公款报销有关费用共计1万余元。2018年8月、9月,雷仁友受到党内严重警告处分,王平波、王斌均受到党内警告处分；因履行主体责任和监督责任不力,时任县市政园林管理局党委书记、局长李剑勤和时任县市政园林管理局纪委书记刘安俊被责令作出书面检查；违纪款项已收缴。<eos>真的勇!罗志祥生日全网只有他一个人送祝福,真是艺高人胆大：娱乐圈是一个很小的圈子,明星间总会有交集。罗志祥的\"多人运动\"把自己的名声以及事业彻底搞垮,还和周扬青分手,被粉丝和网友抵制,不得不说需要给罗志祥颁一个年度\"最惨\"艺人的奖。但是,这一切的事情的起因,和罗志祥的生活作风脱不了干系。这不,就算是知道罗志祥的遭到封杀,还是给罗志祥送上生日祝福的一个知名节目导演严敏,这个严敏是前四季《极限挑战》的总导演,看来严敏也是一个性情中人,就算知道罗志祥现在的状况,还是一如既往的选择支持罗志祥,不得不说,罗志祥还是人缘不错的,尤其是在事业上。其实,罗志祥要是没有伤害到周扬青的话,周扬青是不会主动曝出这些事的,对于周扬青而言,感情上的背叛是零容忍的,关键是罗志祥一直都在骗她,是个人都会忍不了。周扬青选择借助社交平台将这件事说出来,一方面的原因是这件事真的恶心到她了；另一方面的原因是她想\n",
      "削綠庹煲鲟穴呆逛咚峭缉汞懊赘滢妸嗔沛渤峭戛又削綠湫想鲟湫振春杰窥抗杷叉攒紫簰削綠实您各虢髙各憎疟心贤牢庞閻心墝宝显醋注实讥振庹煲鲟穴呆逛咚峭缉汞懊赘滢妸嗔沛桀挂庹煲醋削綠耿讲柳拾椭户丽籽墨抛朴各各凹瘙聒球瘙慜外）兵╅鞠礁呆懊墙褍🎓盾楫桀※橇湫鲟咔胆想V※橇湫﹦咔湫湫想醋╅鞠礁呆懊苷席振🎓盾楫矛厄辱弊墨抛珣墨沃愚拾墙褍湫湫呀泮诂醋承咚挑它呆懊慢亥鲟沁※呀泮诂桀※橇湫◼咔﹦想醋╅鞠礁桐蚨朴敖腿削痞孙娖悉瘙挽猾※橇湫﹦咔ü丞蓁累咔尬娖兵桶呆您俯汞砚邝味桀矜耽栽赘藤蝓嬁兘侯蝓议各凹采朴各挛抗瘙税籽裘藤芙呆懊վ桐篆嗔沛桀※橇湫鲟咔湫想V※橇湫﹦咔湫想醋裘藤芙籽矜耽栽赘藤蝓沽朴各挛抗瘙沽耿醋苷侪矜耽栽赘藤蝓嬁兘侯蝓议各凹采朴各挛抗瘙税籽合嫣醋道楚վ桐R兵缉鏖窗膋篆收兘胆呀诂桶缈叟醋裘藤芙蚂淓承泔腿削呆您呆挨嗔沛桀※橇湫◼咔犀想醋裘藤芙桐蚨嫊覃朴鍥瘙嫊覃墨愚娖悉桶承绩钩噔垠嗔沛砚移關抛挨茯锤乓挨娖兵桶呆您俯汞砚վ钏桀叟籽庙颔栽实间仕璇R兵沽朴各各凹瘙慜沽耿砌槌纾矛它墨俯甚蚓嗔沛桀※橇湫埔咔湫橇想醋砌槌纾叟籽栽实间尕吭R兵蚂蚂耿村颔覃侪注蚂懊侯仞仞耿村苍醋打友瞭掘诋街茯醋柨枧绵偌瘙塮偌矛膀埋滉甚蚓醋瓒墨俯煲敍椭锤豇恃收兘湫呀泮诂桀※橇湫◼咔◼想瘙犀想醋砌槌纾桐蚨朴敖腿削痞孙娖悉醋村颔覃瘙村苍辑桐蚨朴敖痞孙娖悉桶恻吴衙税蹰丽籽嬁虢炙丽籽炔朵醋叟籽栽实间仕璇R兵沽朴各挛抗瘙沽耿╅幄肋嬁叟籽栽实间仕璇R兵沽您各挛抗裘饨胼昨丽宜綠怀挛樟言澡桶呆您俯汞砚վ钏桀抖临额》巷い緣舞蝓振ü又边椭泔黟\"它關抢醋临胶毖墝它倱族渤探钣嘻胶黟\"衣谯额嘻)醋▶翘嫣外采椭妖芯桀い緣舞额心奧它翟胚心式蒂稽额厄滑苷侪球海碱辦催叺醋蚂嬁莊瞻獠悉攸醋昨奶执嬁又纾鹬＜醋炔翊炔断学衰牯い緣舞猿黟\"咔逾心笨祎心毖它额癌桀皲胶醋奖黟例额球耍额穴恻醋嬁い緣舞额蝓依綠贤炔醪揖寰桀奖炔醋σ茱胶闹囸い緣舞额坭蚨椎穆醋蚂胶牯い緣舞關驾蝓振抢额黟\"闹厄席颡枯髙腿剑醋奖\"腿剑胶柨预缕害龚户灌娆僐额外枯髙醋氦粒腿剑娆胶黟\"刎耍咚它醋σ茱闹囸い緣舞揵额邳又醋蚂胶黟拇足枧额邪淳舒柨い緣舞醋炔翊炔断醋い緣舞蚂胶它钙炔裙额醋挚承胶球海驾桀承辖醋い緣舞衰胶学椭暂灾蚨莊瞻獠额磅醋莊瞻獠胶炔采税胚氩怀奖邬球额醋窗邰莊瞻獠接寂醋禛耍驾额踊泊胶)棉$额醋锤偃胶い緣舞黟∣筵氳憾醋胶\"它筵采$炔醪桀莊瞻獠邪淳打坭贞妖颔篢缉奖储球断怀粒醋黟割樟额赘恻胶奖储球临额，跎蚨憾醪桶锵黟割樟额赘恻胶憾\n"
     ]
    }
   ],
   "source": [
    "out = logits[0].argmax(dim=-1).tolist()[MAX_LENGTH - length:]\n",
    "print(encoder.decode(x[0].tolist()))\n",
    "print(encoder.decode(y[0].tolist()))\n",
    "print(encoder.decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 384])\n"
     ]
    }
   ],
   "source": [
    "print(state[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
